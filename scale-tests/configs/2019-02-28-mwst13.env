# Depends on:
# - TEST_NAME
# - TEST_S3_BUCKET
# - TEST_S3_FOLDER

# Cluster configuration ########################################################
# Total CPU quota: 2330
# Total MEM quota: 5500000

CLUSTER_URL="https://mwst13.scaletesting.mesosphe.re"
SECURITY="strict"

# Test configuration ###########################################################

SHOULD_INSTALL_INFRASTRUCTURE=true
SHOULD_INSTALL_NON_GPU_DISPATCHERS=true
SHOULD_INSTALL_GPU_DISPATCHERS=true
SHOULD_RUN_FAILING_STREAMING_JOBS=false
SHOULD_RUN_FINITE_STREAMING_JOBS=true
SHOULD_RUN_INFINITE_STREAMING_JOBS=true
SHOULD_RUN_BATCH_JOBS=true
SHOULD_RUN_GPU_BATCH_JOBS=true
SHOULD_UNINSTALL_INFRASTRUCTURE_AT_THE_END=false

# Infrastructure configuration #################################################

SERVICE_NAMES_PREFIX="${TEST_NAME}/"
INFRASTRUCTURE_OUTPUT_FILE="infrastructure.json"
KAFKA_ZOOKEEPER_CONFIG='scale-tests/configs/kafka-zookeeper-options.json'
KAFKA_CLUSTER_COUNT=1
KAFKA_CONFIG='scale-tests/configs/kafka-options.json'
CASSANDRA_CLUSTER_COUNT=1
CASSANDRA_CONFIG='scale-tests/configs/cassandra-options.json'

# Spark configuration ##########################################################

# Note: leaving the Spark executor Docker image empty so that
# executors inherit the image used for dispatchers.
SPARK_EXECUTOR_DOCKER_IMAGE=
# Note: leaving the Spark package repository empty so that it uses the
# one available in the cluster's Universe. For this test, it should
# be "Spark 2.6.0-2.3.2".
SPARK_PACKAGE_REPO=

# Non-GPU dispatchers configuration ############################################

NON_GPU_NUM_DISPATCHERS=50
NON_GPU_DISPATCHERS_OUTPUT_FILE="non-gpu-dispatchers.out"
NON_GPU_DISPATCHERS_JSON_OUTPUT_FILE="${NON_GPU_DISPATCHERS_OUTPUT_FILE}-dispatchers.json" # NOTE: this name is built internally by the deploy-dispatchers.py script.
NON_GPU_QUOTA_DRIVERS_CPUS=20
NON_GPU_QUOTA_DRIVERS_MEM=50000
NON_GPU_QUOTA_EXECUTORS_CPUS=25
NON_GPU_QUOTA_EXECUTORS_MEM=40000
# Total quota:
# - driver:
#   - 1000 cpus
#   - 2500000 mem
# - executor:
#   - 1250 cpus
#   - 2000000 mem

# GPU dispatchers configuration ################################################

GPU_NUM_DISPATCHERS=5
GPU_DISPATCHERS_OUTPUT_FILE="gpu-dispatchers.out"
GPU_DISPATCHERS_JSON_OUTPUT_FILE="${GPU_DISPATCHERS_OUTPUT_FILE}-dispatchers.json" # NOTE: this name is built internally by the deploy-dispatchers.py script.
GPU_QUOTA_DRIVERS_CPUS=16
GPU_QUOTA_DRIVERS_MEM=200000
GPU_QUOTA_DRIVERS_GPUS=
GPU_QUOTA_EXECUTORS_CPUS=
GPU_QUOTA_EXECUTORS_MEM=
GPU_QUOTA_EXECUTORS_GPUS=
# NOTE: to test Core teamâ€™s hypothesis of GPU quota negatively impacting spark
# launch rates, we remove the GPU quota entirely from all executor roles.
GPU_REMOVE_EXECUTORS_ROLES_QUOTAS=true
# Total quota:
# - driver:
#   - 80 cpus
#   - 1000000 mem
#   - 0 gpus
# - executor:
#   - 0 cpus
#   - 0 mem
#   - 0 gpus

# Common streaming jobs configuration ##########################################

TEST_ASSEMBLY_JAR_URL='http://infinity-artifacts.s3.amazonaws.com/scale-tests/dcos-spark-scala-tests-assembly-20180612-5fa9420.jar'
NUM_DISPATCHERS="$((${NON_GPU_NUM_DISPATCHERS} + ${GPU_NUM_DISPATCHERS}))"
DISPATCHERS_JSON_OUTPUT_FILE="all-dispatchers.json"

# Failing streaming jobs configuration #########################################

FAILING_SUBMISSIONS_OUTPUT_FILE="failing-submissions.out"
FAILING_NUM_PRODUCERS_PER_KAFKA="${NON_GPU_NUM_DISPATCHERS}"
FAILING_NUM_CONSUMERS_PER_PRODUCER=1
FAILING_PRODUCER_NUMBER_OF_WORDS=7692
FAILING_PRODUCER_WORDS_PER_SECOND=1
FAILING_PRODUCER_SPARK_CORES_MAX=2
FAILING_PRODUCER_SPARK_EXECUTOR_CORES=2
FAILING_CONSUMER_BATCH_SIZE_SECONDS=10
FAILING_CONSUMER_SPARK_CORES_MAX=1
FAILING_CONSUMER_SPARK_EXECUTOR_CORES=1

# Finite streaming jobs configuration ##########################################

FINITE_SUBMISSIONS_OUTPUT_FILE="finite-submissions.out"
FINITE_NUM_PRODUCERS_PER_KAFKA="${NON_GPU_NUM_DISPATCHERS}" # 1 Kafka and 50 dispatchers -> 50 producers.
FINITE_NUM_CONSUMERS_PER_PRODUCER=1 # 50 producers -> 50 consumers.
# 50 producers + 50 consumers = 100 total finite streaming jobs
FINITE_PRODUCER_NUMBER_OF_WORDS=7692
FINITE_PRODUCER_WORDS_PER_SECOND=1
# 7692 words / 1 word per second -> ~2h runtime.
FINITE_PRODUCER_SPARK_CORES_MAX=2
FINITE_PRODUCER_SPARK_EXECUTOR_CORES=2
FINITE_CONSUMER_BATCH_SIZE_SECONDS=10
FINITE_CONSUMER_SPARK_CORES_MAX=1
FINITE_CONSUMER_SPARK_EXECUTOR_CORES=1

# Infinite streaming jobs configuration ########################################

INFINITE_SUBMISSIONS_OUTPUT_FILE="infinite-submissions.out"
INFINITE_NUM_PRODUCERS_PER_KAFKA="${NON_GPU_NUM_DISPATCHERS}" # 1 Kafka and 50 dispatchers -> 50 producers.
INFINITE_NUM_CONSUMERS_PER_PRODUCER=1 # 50 producers -> 50 consumers.
# 50 producers + 50 consumers = 100 total infinite streaming jobs
INFINITE_PRODUCER_NUMBER_OF_WORDS=0
INFINITE_PRODUCER_WORDS_PER_SECOND=1
INFINITE_PRODUCER_SPARK_CORES_MAX=2
INFINITE_PRODUCER_SPARK_EXECUTOR_CORES=2
INFINITE_CONSUMER_BATCH_SIZE_SECONDS=10
INFINITE_CONSUMER_SPARK_CORES_MAX=1
INFINITE_CONSUMER_SPARK_EXECUTOR_CORES=1

# Batch jobs configuration #####################################################

NON_GPU_DISPATCHERS_JSON_OUTPUT_FILE_URL="https://${TEST_S3_BUCKET}.s3.amazonaws.com/${TEST_S3_FOLDER}/${NON_GPU_DISPATCHERS_JSON_OUTPUT_FILE}"

BATCH_APP_ID="/${SERVICE_NAMES_PREFIX}batch-workload"
BATCH_SCRIPT_CPUS=6
BATCH_SCRIPT_MEM=12288
BATCH_SUBMITS_PER_MIN=13
BATCH_SPARK_BUILD_BRANCH=master

# Batch GPU jobs configuration #################################################

GPU_DISPATCHERS_JSON_OUTPUT_FILE_URL="https://${TEST_S3_BUCKET}.s3.amazonaws.com/${TEST_S3_FOLDER}/${GPU_DISPATCHERS_JSON_OUTPUT_FILE}"

GPU_APP_ID="/${SERVICE_NAMES_PREFIX}gpu-batch-workload"
GPU_SCRIPT_CPUS=2
GPU_SCRIPT_MEM=4096
GPU_DOCKER_IMAGE='samvantran/spark-dcos-gpu:metrics'
GPU_SUBMITS_PER_MIN=5
GPU_MAX_NUM_DISPATCHERS=${GPU_NUM_DISPATCHERS}
GPU_SPARK_CORES_MAX=4
GPU_SPARK_MESOS_EXECUTOR_GPUS=4
GPU_SPARK_MESOS_MAX_GPUS=4
GPU_SPARK_BUILD_BRANCH=master
